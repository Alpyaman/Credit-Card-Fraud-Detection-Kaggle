# ğŸ›¡ï¸ Production-Ready Credit Card Fraud Detection System

![MLOps Pipeline](https://img.shields.io/badge/MLOps-Production--Ready-brightgreen)
![Docker](https://img.shields.io/badge/Docker-Enabled-blue)
![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub%20Actions-yellow)
![Cloud](https://img.shields.io/badge/Cloud-AWS%20%7C%20GCP-orange)
![Tests](https://img.shields.io/badge/Tests-28%2F28%20Passing-success)

A complete end-to-end MLOps pipeline for credit card fraud detection, featuring containerization, automated CI/CD, model monitoring, and cloud deployment capabilities.

---

## ğŸ¯ Project Overview

This project demonstrates a **production-grade MLOps system** with:

- âœ… **Trained Model** (ROC-AUC: 0.9508)
- âœ… **FastAPI REST API** for real-time predictions
- âœ… **Containerization** with Docker
- âœ… **CI/CD Pipeline** with GitHub Actions
- âœ… **Model Monitoring** dashboard (Python 3.11 compatible)
- âœ… **Cloud Deployment** configs (AWS ECS, GCP Cloud Run)
- âœ… **Comprehensive Testing** (28/28 tests passing)
- âœ… **Complete Documentation**

---

## ğŸ“ Project Structure

```
Credit-Card-Fraud-Detection-Kaggle/
â”‚
â”œâ”€â”€ api/                          # FastAPI application
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py                   # API endpoints and model serving
â”‚
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocess.py             # Data preprocessing module
â”‚   â”œâ”€â”€ train.py                  # Model training pipeline
|   â”œâ”€â”€ download_data.py          # Dataset download script
|   â””â”€â”€ create_monitoring_data.py # Create monitoring data

â”‚
â”œâ”€â”€ monitoring/                   # Monitoring dashboards
â”‚   â”œâ”€â”€ dashboard.py              # Evidently AI dashboard
â”‚   â”œâ”€â”€ dashboard_simple.py       # Simplified dashboard (Python 3.11)
â”‚   â””â”€â”€ Dockerfile                # Monitoring container
â”‚
â”œâ”€â”€ tests/                        # Test suite (28 tests)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_preprocess.py        # Preprocessing tests
â”‚   â”œâ”€â”€ test_api.py               # API tests
|   â””â”€â”€ test_api_local.py         # API testing script

â”‚
â”œâ”€â”€ cloud/                        # Cloud deployment configs
â”‚   â”œâ”€â”€ aws-ecs.tf                # AWS ECS Terraform
â”‚   â””â”€â”€ gcp-cloudrun.tf           # GCP Cloud Run Terraform
â”‚
â”œâ”€â”€ .github/workflows/            # CI/CD pipelines
â”‚   â””â”€â”€ ci-cd.yml                 # GitHub Actions workflow
â”‚
â”œâ”€â”€ models/                       # Trained models
â”‚   â”œâ”€â”€ fraud_model.pkl           # Trained LightGBM model
â”‚   â”œâ”€â”€ preprocessor.pkl          # Fitted preprocessor
â”‚   â”œâ”€â”€ model_metadata.json       # Model metadata & metrics
â”‚   â””â”€â”€ *.png                     # Visualization plots
â”‚
â”œâ”€â”€ data/                         # Data directory
â”‚   â”œâ”€â”€ creditcard.csv            # Main dataset (143MB)
â”‚   â”œâ”€â”€ reference_data.csv        # Reference data for monitoring
â”‚   â”œâ”€â”€ production_predictions.csv # Sample production data
â”‚   â””â”€â”€ test_sample.csv           # Test samples
â”‚
â”œâ”€â”€ Dockerfile                    # Main application container
â”œâ”€â”€ docker-compose.yml            # Multi-container orchestration
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ setup.py                      # Package setup
â”‚
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ QUICKSTART.md                 # Quick start guide
â””â”€â”€ WHATS_NEXT.md                 # Learning roadmap
```

---

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/Alpyaman/Credit-Card-Fraud-Detection-Kaggle.git
cd Credit-Card-Fraud-Detection-Kaggle
```

### 2. Setup Environment

```bash
# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.venv\Scripts\activate
# Linux/Mac:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Install project in development mode
pip install -e .
```

### 3. Download Data and Train Model

```bash
# Download the dataset
python download_data.py

# Train the model
python src/train.py

# Create monitoring data
python create_monitoring_data.py
```

**Output:**
- `models/fraud_model.pkl` - Trained model (ROC-AUC: 0.9508)
- `models/preprocessor.pkl` - Data preprocessor
- `models/model_metadata.json` - Metrics and metadata
- Visualization plots (ROC curve, PR curve, feature importance)

### 4. Run Tests

```bash
# Run all tests
pytest tests/ -v

# Expected: All 28 tests pass âœ…
```

### 5. Start the API

```bash
# Start FastAPI server
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

**API Endpoints:**
- http://localhost:8000 - Root
- http://localhost:8000/docs - Interactive API docs
- http://localhost:8000/health - Health check
- http://localhost:8000/predict - Single prediction
- http://localhost:8000/predict/batch - Batch predictions
- http://localhost:8000/model/info - Model information

### 6. Test the API

```bash
# In another terminal, run the test script
python test_api_local.py

# Expected: All 5 API tests pass âœ…
```

### 7. Run Monitoring Dashboard

```bash
# Start the monitoring dashboard
streamlit run monitoring/dashboard_simple.py
```

Visit: http://localhost:8501

**Dashboard Features:**
- Overview: Model metrics and data summary
- Data Drift: Drift detection with statistical tests
- Model Performance: Performance metrics and trends
- Predictions: Recent prediction analysis

---

## ğŸ“š Documentation

- **[QUICKSTART.md](QUICKSTART.md)** - Step-by-step getting started guide
- **[WHATS_NEXT.md](WHATS_NEXT.md)** - Next steps and learning path

---

## ğŸ› ï¸ Technology Stack

**Machine Learning:**
- LightGBM - Gradient boosting framework
- Scikit-learn - Preprocessing and metrics
- Pandas, NumPy - Data manipulation

**API & Web:**
- FastAPI - REST API framework
- Uvicorn - ASGI server
- Pydantic - Data validation
- Streamlit - Monitoring dashboard

**DevOps & MLOps:**
- Docker - Containerization
- Docker Compose - Multi-container orchestration
- GitHub Actions - CI/CD
- Pytest - Testing framework

**Cloud & Infrastructure:**
- Terraform - Infrastructure as Code
- AWS ECS - Container orchestration
- GCP Cloud Run - Serverless containers

**Monitoring:**
- Plotly - Interactive visualizations
- SciPy - Statistical analysis
- Custom drift detection

---

## ğŸ“ˆ Project Highlights

âœ… **End-to-End Pipeline**: Complete flow from data â†’ model â†’ API â†’ deployment  
âœ… **Production Ready**: Tests, monitoring, documentation all in place  
âœ… **Cloud Native**: Containerized and ready for any cloud platform  
âœ… **Automated**: CI/CD handles testing, building, and deployment  
âœ… **Maintainable**: Well-structured, tested, and documented code  
âœ… **Scalable**: Can handle production workloads with proper deployment  

---

## ğŸ“ Learning Outcomes

This project demonstrates:

- âœ… Machine Learning model development and evaluation
- âœ… REST API design and implementation
- âœ… Containerization with Docker
- âœ… CI/CD pipeline setup with GitHub Actions
- âœ… Model monitoring and drift detection
- âœ… Cloud deployment (AWS & GCP)
- âœ… Infrastructure as Code with Terraform
- âœ… Comprehensive testing strategies
- âœ… Production-ready MLOps practices

### Option 1: Docker Compose (Recommended)

```bash
# Build and start all services
docker-compose up -d

# Services:
# - API: http://localhost:8000
# - Monitoring: http://localhost:8501

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Option 2: Docker Only

```bash
# Build the image
docker build -t fraud-detection-api:latest .

# Run the container
docker run -d \
  -p 8000:8000 \
  -v $(pwd)/models:/app/models \
  --name fraud-detection \
  fraud-detection-api:latest

# Check logs
docker logs fraud-detection

# Stop container
docker stop fraud-detection
```

---

## ğŸ§ª Testing

### Run All Tests

```bash
# Run all tests with verbose output
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src --cov=api --cov-report=html

# Run specific test file
pytest tests/test_api.py -v
```

**Test Coverage:**
- âœ… 14 API tests (endpoints, validation, error handling)
- âœ… 14 preprocessing tests (transformations, edge cases)
- âœ… All 28 tests passing

---

## ğŸ“Š Model Performance

**Training Results:**
- **ROC-AUC Score**: 0.9508
- **Precision**: 0.8537
- **Recall**: 0.7143
- **F1-Score**: 0.7778

**Model Details:**
- Algorithm: LightGBM Classifier
- Features: 34 (28 original + 5 outlier flags + scaled amount)
- Best Parameters:
  - learning_rate: 0.05
  - max_depth: 5
  - n_estimators: 100
  - subsample: 0.8

---

## ğŸ“Š API Usage

### Health Check

```bash
curl http://localhost:8000/health
```

### Single Prediction

```bash
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "Time": 0.0,
    "V1": -1.359807,
    "V2": -0.072781,
    ...
    "Amount": 149.62
  }'
```

### Batch Prediction

```bash
curl -X POST "http://localhost:8000/predict/batch" \
  -H "Content-Type: application/json" \
  -d '{
    "transactions": [
      {...transaction1...},
      {...transaction2...}
    ]
  }'
```

### Python Client Example

```python
import requests

# Prepare transaction data
transaction = {
    "Time": 0.0,
    "V1": -1.359807,
    # ... other features ...
    "Amount": 149.62
}

# Make prediction
response = requests.post(
    "http://localhost:8000/predict",
    json=transaction
)

result = response.json()
print(f"Fraud: {result['is_fraud']}")
print(f"Probability: {result['fraud_probability']:.2%}")
```

---

## ğŸ”„ CI/CD Pipeline

The project includes a complete GitHub Actions workflow that:

1. **Runs Tests** - Unit tests and code quality checks
2. **Builds Docker Image** - Containerizes the application
3. **Retrains Model** - When triggered or new data added
4. **Deploys to Cloud** - AWS ECS or GCP Cloud Run

### Trigger Manual Retrain

```bash
# Commit with retrain flag
git commit -m "[retrain] Update model with new data"
git push
```

### Required GitHub Secrets

Add these secrets to your GitHub repository:

```
# Docker Hub
DOCKER_USERNAME
DOCKER_PASSWORD

# AWS
AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY

# GCP
GCP_SA_KEY
GCP_PROJECT_ID
```

---

## â˜ï¸ Cloud Deployment

### AWS ECS Deployment

1. **Setup Infrastructure**

```bash
cd cloud
terraform init
terraform plan
terraform apply
```

2. **Push Image to ECR**

```bash
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <account-id>.dkr.ecr.us-east-1.amazonaws.com
docker tag fraud-detection-api:latest <account-id>.dkr.ecr.us-east-1.amazonaws.com/fraud-detection-api:latest
docker push <account-id>.dkr.ecr.us-east-1.amazonaws.com/fraud-detection-api:latest
```

### GCP Cloud Run Deployment

1. **Build and Deploy**

```bash
gcloud builds submit --tag gcr.io/<project-id>/fraud-detection-api
gcloud run deploy fraud-detection-api \
  --image gcr.io/<project-id>/fraud-detection-api \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated
```

---

## ğŸ“ˆ Model Monitoring

### Start Monitoring Dashboard

```bash
streamlit run monitoring/dashboard.py
```

The dashboard provides:

- ğŸ“Š **Data Drift Detection** - Tracks feature distribution changes
- ğŸ¯ **Model Performance Metrics** - ROC-AUC, Precision, Recall
- ğŸ” **Prediction Analysis** - Recent predictions and patterns
- âš ï¸ **Alerts** - Automated alerts for performance degradation

---

## ğŸ§ª Testing

### Run All Tests

```bash
pytest tests/ -v
```

### Run with Coverage

```bash
pytest tests/ -v --cov=src --cov=api --cov-report=html
```

### Run Specific Test File

```bash
pytest tests/test_api.py -v
```

---

## ğŸ“Š Model Performance

| Metric | Value |
|--------|-------|
| **ROC-AUC** | 0.9900 |
| **Precision (Fraud)** | 85.37% |
| **Recall (Fraud)** | 71.43% |
| **F1-Score** | 77.78% |
| **Accuracy** | 99.93% |

---

## ğŸ› ï¸ Development Workflow

1. **Create Feature Branch**
```bash
git checkout -b feature/your-feature
```

2. **Make Changes and Test**
```bash
pytest tests/ -v
flake8 src/ api/
```

3. **Commit and Push**
```bash
git add .
git commit -m "Description of changes"
git push origin feature/your-feature
```

4. **Create Pull Request** - CI/CD will run automatically

---

## ğŸ” Security Considerations

- âœ… API authentication (implement as needed)
- âœ… HTTPS/TLS encryption in production
- âœ… Environment variable management
- âœ… Secrets management (AWS Secrets Manager, GCP Secret Manager)
- âœ… Container security scanning
- âœ… Regular dependency updates

---

## ğŸ“ Environment Variables

Create a `.env` file for local development:

```bash
MODEL_PATH=models/fraud_model.pkl
PREPROCESSOR_PATH=models/preprocessor.pkl
LOG_LEVEL=INFO
API_KEY=your-secret-key  # If implementing authentication
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ“ License

This project is for educational purposes as part of MLOps learning.

---

## ğŸ“¦ Support

For questions or issues:
- Check the documentation files (QUICKSTART.md, etc.)
- Open an issue on GitHub
- Review [WHATS_NEXT.md](WHATS_NEXT.md) for common scenarios

---

## ğŸ‰ Acknowledgments

- Dataset: [Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) from Kaggle
- Inspired by production MLOps best practices

---

**Built with â¤ï¸ for learning MLOps**

This project demonstrates:

- **MLOps Best Practices** - Production ML workflows
- **Docker & Containerization** - Application packaging
- **REST API Design** - FastAPI implementation
- **CI/CD Pipelines** - Automated testing and deployment
- **Cloud Platforms** - AWS and GCP services
- **Model Monitoring** - Drift detection and alerting

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## ğŸ‘¤ Author

**Alp Yaman**  
Intermediate Data Scientist - AI Enthusiast

- GitHub: [@alpyaman](https://github.com/alpyaman)

---

## ğŸ™ Acknowledgments

- Dataset: [Kaggle Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
- Monitoring: [Evidently AI](https://www.evidentlyai.com/)
- Framework: [FastAPI](https://fastapi.tiangolo.com/)

---

## ğŸ“ Support

For issues and questions:
- Create an [Issue](https://github.com/Alpyaman/Credit-Card-Fraud-Detection-Kaggle/issues)
- Email: alpyaman3@gmail.com

---

**Ready to deploy to production! ğŸš€**
